{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]\n",
    "\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "transformers.utils.logging.set_verbosity_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1gDuwzgSk8rxUnQKR0Hyn70A5SMt1l4_9' -O data.zip\n",
    "\n",
    "!unzip data.zip\n",
    "\n",
    "\n",
    "directory = './data/investopedia'\n",
    "dataframes = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dataframes.append(df)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "df = df.loc[df['Title'] != 'No Title Found']\n",
    "text = pd.Series(df['Title'] + df['Summary'], dtype=str).reset_index(drop=True)\n",
    "# Calculate the lengths of the strings\n",
    "lengths = text.str.len()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(lengths, bins=30, edgecolor='black')\n",
    "plt.xlabel('Length of Text')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "import json\n",
    "\n",
    "# Enable logging for debugging\n",
    "transformers.utils.logging.set_verbosity_debug()\n",
    "\n",
    "# Assuming your data is in a DataFrame called `text`\n",
    "df = text\n",
    "\n",
    "# Tokenizer and Model Initialization\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Dataset Preprocessing\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "train_texts, test_texts = train_test_split(df.tolist(), test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts})\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "\n",
    "# Define Data Collator for Masked Language Modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the compute_metrics function to calculate perplexity\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Convert logits and labels from NumPy arrays to PyTorch tensors\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "    perplexity = torch.exp(loss)\n",
    "    return {\"perplexity\": perplexity.item()}\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and Save the Model\n",
    "trainer.train()\n",
    "trainer.save_model(\"./finbert_finetuned\")\n",
    "tokenizer.save_pretrained(\"./finbert_finetuned\")\n",
    "\n",
    "# Save Trainer State\n",
    "trainer.state.save_to_json(\"./finbert_finetuned/trainer_state.json\")\n",
    "\n",
    "# Save Training Arguments\n",
    "with open(\"./finbert_finetuned/training_args.json\", \"w\") as f:\n",
    "    json.dump(training_args.to_dict(), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Download stopwords if not already available\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./finbert_finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def mask_word_tokens(text, tokenizer, mask_probability=0.10):\n",
    "    \"\"\"\n",
    "    Mask tokens randomly, excluding punctuation, stop words, and one-character words.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens_with_pos = pos_tag(tokens)\n",
    "    \n",
    "    # Exclude stopwords, punctuation, and one-character words\n",
    "    eligible_tokens = [\n",
    "        i for i, (token, pos) in enumerate(tokens_with_pos)\n",
    "        if token.lower() not in stop_words and len(token) > 1 and token.isalnum()\n",
    "    ]\n",
    "    \n",
    "    num_tokens_to_mask = max(1, int(len(eligible_tokens) * mask_probability))\n",
    "    mask_indices = random.sample(eligible_tokens, num_tokens_to_mask)\n",
    "    \n",
    "    masked_tokens = tokens.copy()\n",
    "    for idx in mask_indices:\n",
    "        masked_tokens[idx] = tokenizer.mask_token\n",
    "    \n",
    "    return tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "\n",
    "# Function to predict masked tokens\n",
    "def predict_masked_tokens(test_text, model, tokenizer):\n",
    "    masked_text = mask_word_tokens(test_text, tokenizer, mask_probability=0.10)\n",
    "    print(f\"Original text ---- {test_text}\")\n",
    "    print(f\"Masked text ---- {masked_text}\")\n",
    "    \n",
    "    inputs = tokenizer(masked_text, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Find the indices of the masked tokens\n",
    "    mask_token_indices = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # Decode the predicted tokens\n",
    "    predicted_tokens = []\n",
    "    for index in mask_token_indices:\n",
    "        predicted_token_id = logits[0, index].argmax(axis=-1)\n",
    "        predicted_token = tokenizer.decode(predicted_token_id)\n",
    "        predicted_tokens.append(predicted_token)\n",
    "    \n",
    "    # Replace the [MASK] tokens with the predicted tokens\n",
    "    output_text = masked_text\n",
    "    for predicted_token in predicted_tokens:\n",
    "        output_text = output_text.replace('[MASK]', predicted_token, 1)\n",
    "    \n",
    "    print(f\"Predicted text: {output_text}\")\n",
    "\n",
    "# Example test text\n",
    "test_text = (\n",
    "    \"Unsecured Debt Definition: Unsecured debts are loans that are not collateralized.\"\n",
    "    \" They generally require higher interest rates because they offer the lender limited protection against default.\"\n",
    "    \" Lenders can mitigate this risk by reporting defaults to credit rating agencies.\"\n",
    ")\n",
    "\n",
    "# Run the prediction\n",
    "predict_masked_tokens(test_text, model, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
