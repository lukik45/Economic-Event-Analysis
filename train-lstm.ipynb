{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMpnDaEuicyEhFkO0i5llHq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9MqbCOLacD7","outputId":"82283d9f-7727-46dc-e759-6a7979ac13af","executionInfo":{"status":"ok","timestamp":1716651094092,"user_tz":-120,"elapsed":83182,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.30.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n","Collecting datasets\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n","Collecting langchain\n","  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n","  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n","  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.0\n","    Uninstalling packaging-24.0:\n","      Successfully uninstalled packaging-24.0\n","Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langsmith-0.1.63 orjson-3.10.3 packaging-23.2\n","Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"]}],"source":["!pip install transformers[torch]\n","# !pip install torch\n","!pip install datasets\n","!pip install langchain scikit-learn\n","!pip install gdown"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xnioL2ak-T3","executionInfo":{"status":"ok","timestamp":1716651116288,"user_tz":-120,"elapsed":22201,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"outputId":"27a8bc73-9514-4d28-a628-cf8ae2869730"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\n","import gdown\n","\n","url = 'https://drive.google.com/uc?id=1Ia_4o4JJDoyIkPcbMC_eZbWE_1DpuerL'\n","output = 'data.zip'\n","gdown.download(url, output, quiet=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"1bjoN3WMlziG","executionInfo":{"status":"ok","timestamp":1716651128917,"user_tz":-120,"elapsed":12636,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"outputId":"2cb5a41d-ef1b-4c49-b3e8-fdbf06fe1ccb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1Ia_4o4JJDoyIkPcbMC_eZbWE_1DpuerL\n","To: /content/data.zip\n","100%|██████████| 12.2M/12.2M [00:00<00:00, 22.1MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'data.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","with zipfile.ZipFile(output, 'r') as zip_ref:\n","    zip_ref.extractall('/content')\n","\n"],"metadata":{"id":"cuf9tpItl2Fv","executionInfo":{"status":"ok","timestamp":1716651129461,"user_tz":-120,"elapsed":556,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Adjust the path below if the speech.csv is inside a folder from the zip\n","csv_file = '/content/data/updated_file_pre_2006.csv'\n","data = pd.read_csv(csv_file)\n","\n","len(data)\n","data = data[:180]\n","len(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1RZIdUrmHwP","executionInfo":{"status":"ok","timestamp":1716651131464,"user_tz":-120,"elapsed":2006,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"outputId":"2d39a169-5e9e-4f02-db3e-2180a450c1f1"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["180"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from google.colab import drive\n","from transformers import AutoTokenizer, AutoModelForMaskedLM, BertModel\n","import torch\n","import random\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize, pos_tag\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Path to the fine-tuned model on Google Drive\n","model_path = \"/content/drive/My Drive/finbert_finetuned\"\n","\n","# Load the fine-tuned model and tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = BertModel.from_pretrained(model_path)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)  # Ensure the model is on the correct device\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rv6HrU-3gHR3","executionInfo":{"status":"ok","timestamp":1716651147761,"user_tz":-120,"elapsed":16301,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"outputId":"4a8d506c-79c5-46b2-af5b-f99049a81d9a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/finbert_finetuned and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSdpaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load dataset\n","df = data\n","df = df[['content', 'Classification']]\n","\n","# Transform classification labels\n","def transform_classification_label(label):\n","    return 1 if label == 1.0 else 0\n","\n","df['classification'] = df['Classification'].apply(transform_classification_label)"],"metadata":{"id":"0bqWyT8ZhcEu","executionInfo":{"status":"ok","timestamp":1716651147761,"user_tz":-120,"elapsed":28,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bffe9525-ff0e-405c-cf89-46eb3ae750cc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-5f0e532300ea>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['classification'] = df['Classification'].apply(transform_classification_label)\n"]}]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Token count splitter\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=512,\n","    chunk_overlap=50,\n","    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"]\n",")\n","\n","def split_text(text):\n","    chunks = text_splitter.split_text(text)\n","    return chunks\n","\n","# Apply text splitting to the dataframe\n","df['chunks'] = df['content'].apply(split_text)\n"],"metadata":{"id":"D8sdbNK6heaF","executionInfo":{"status":"ok","timestamp":1716651148302,"user_tz":-120,"elapsed":564,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e01fd8e-a17a-4567-c312-c45e215f5552"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-983f467ded78>:15: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['chunks'] = df['content'].apply(split_text)\n"]}]},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","\n","def get_cls_embeddings(texts, tokenizer, model, device):\n","    # Tokenize the input texts and prepare inputs\n","    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","    # Move all inputs to the specified device (GPU or CPU)\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    # Perform model inference without calculating gradients\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        # Extract the CLS token's embeddings from the last hidden state\n","        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n","    # Move embeddings back to CPU for further processing/storage (if necessary)\n","    return cls_embeddings.cpu().numpy()\n","\n","# Load model and tokenizer onto the specified device\n","model.to(device)  # Make sure your model is also on the same device before the loop\n","\n","# Prepare lists to hold the embeddings and labels\n","cls_embeddings_list = []\n","labels_list = []\n","\n","# Iterate over the DataFrame to process each row\n","for _, row in tqdm(df.iterrows(), total=len(df)):\n","    chunks = row['chunks']  # Assume 'chunks' contains a list of text segments\n","    label = row['classification']\n","\n","    # Process each list of text chunks\n","    embeddings = get_cls_embeddings(chunks, tokenizer, model, device)\n","    cls_embeddings_list.extend(embeddings)\n","    labels_list.extend([label] * len(embeddings))\n","    torch.cuda.empty_cache()  # Clear unused memory\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cw_X6yyAhgN1","executionInfo":{"status":"ok","timestamp":1716651326731,"user_tz":-120,"elapsed":178433,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"outputId":"9562e91b-9ae8-4cde-ecd8-cb9369b8bfa8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 180/180 [02:58<00:00,  1.01it/s]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Convert lists to numpy arrays\n","X = np.array(cls_embeddings_list)\n","y = np.array(labels_list)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define a custom dataset\n","class TextDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","# Create DataLoader objects\n","train_dataset = TextDataset(X_train, y_train)\n","test_dataset = TextDataset(X_test, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"id":"IUZxecPYYJhy","executionInfo":{"status":"ok","timestamp":1716651327316,"user_tz":-120,"elapsed":605,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n","        super(LSTMClassifier, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)  # Add batch dimension for LSTM\n","        lstm_out, _ = self.lstm(x)\n","        out = lstm_out[:, -1, :]  # Get the last output of LSTM\n","        out = self.fc(out)\n","        return out\n","\n","# Instantiate the model\n","input_dim = X.shape[1]\n","hidden_dim = 128\n","output_dim = 2  # Positive or Negative\n","num_layers = 2\n","\n","lstm_model = LSTMClassifier(input_dim, hidden_dim, output_dim, num_layers).to(device)\n"],"metadata":{"id":"Wju_mnMpYKJt","executionInfo":{"status":"ok","timestamp":1716651327316,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n","\n","# Training function\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()  # Zero gradients\n","\n","            outputs = model(inputs)  # Forward pass\n","\n","            loss = criterion(outputs, labels)  # Compute loss\n","\n","            loss.backward()  # Backward pass\n","            optimizer.step()  # Optimize\n","\n","            total_loss += loss.item()\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n","\n","# Train the model\n","train_model(lstm_model, train_loader, criterion, optimizer, num_epochs=10)\n"],"metadata":{"id":"xMLnQk3FYLkP","executionInfo":{"status":"ok","timestamp":1716651423888,"user_tz":-120,"elapsed":96575,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd03dedb-07dc-40c3-fa4d-ab6bd69c4d04"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.5005\n","Epoch [2/10], Loss: 0.2626\n","Epoch [3/10], Loss: 0.1622\n","Epoch [4/10], Loss: 0.1204\n","Epoch [5/10], Loss: 0.0981\n","Epoch [6/10], Loss: 0.0833\n","Epoch [7/10], Loss: 0.0719\n","Epoch [8/10], Loss: 0.0659\n","Epoch [9/10], Loss: 0.0572\n","Epoch [10/10], Loss: 0.0534\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, accuracy_score\n","\n","# Evaluation function\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    report = classification_report(all_labels, all_preds)\n","\n","    print(f\"Loss: {total_loss / len(test_loader):.4f}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Classification Report:\\n{report}\")\n","\n","# Evaluate the LSTM model\n","evaluate_model(lstm_model, test_loader, criterion)\n"],"metadata":{"id":"T93m6FOYYOVG","executionInfo":{"status":"ok","timestamp":1716651424931,"user_tz":-120,"elapsed":1048,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3b9077d-c51f-4c1c-d38a-3d5e8af35ac8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 0.1714\n","Accuracy: 0.9454\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.99      0.95     15187\n","           1       0.98      0.90      0.94     14031\n","\n","    accuracy                           0.95     29218\n","   macro avg       0.95      0.94      0.95     29218\n","weighted avg       0.95      0.95      0.95     29218\n","\n"]}]},{"cell_type":"code","source":["path_to_save_model = '/content/drive/My Drive/nlp_lstm_finetuned.pth'\n","torch.save(lstm_model.state_dict(), path_to_save_model)\n","print(f\"Model saved to {path_to_save_model}\")\n"],"metadata":{"id":"e53QASZud_h5","executionInfo":{"status":"ok","timestamp":1716651426391,"user_tz":-120,"elapsed":1462,"user":{"displayName":"Mc Wheeler","userId":"15705924126276376119"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"149be81d-2a28-4b5e-8ede-00db1df7a9bd"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to /content/drive/My Drive/nlp_lstm_finetuned.pth\n"]}]}]}