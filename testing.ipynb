{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install gdown package\n",
        "!pip install gdown\n",
        "\n",
        "# Download the file from Google Drive\n",
        "!gdown --id 1K-_LJP2Ux3KtTtG98xSkKHZUi2q_xNcq -O models.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip models.zip -d /content/models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM7huECsUtGv",
        "outputId": "1b2d5492-bcca-4832-bd28-17fd0e227ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1K-_LJP2Ux3KtTtG98xSkKHZUi2q_xNcq\n",
            "From (redirected): https://drive.google.com/uc?id=1K-_LJP2Ux3KtTtG98xSkKHZUi2q_xNcq&confirm=t&uuid=5c0b6e67-9c18-4732-aade-de9e98d418f6\n",
            "To: /content/models.zip\n",
            "100% 408M/408M [00:03<00:00, 119MB/s]\n",
            "Archive:  models.zip\n",
            "replace /content/models/config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: /content/models/config.json  \n",
            "  inflating: /content/models/special_tokens_map.json  \n",
            "  inflating: /content/models/tokenizer.json  \n",
            "  inflating: /content/models/model.safetensors  \n",
            "  inflating: /content/models/tokenizer_config.json  \n",
            "  inflating: /content/models/vocab.txt  \n",
            "  inflating: /content/models/nlp_lstm_finetuned.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, pos_tag\n",
        "import os\n",
        "\n",
        "# Path to the fine-tuned model on Google Drive\n",
        "model_path = \"/content/models\"\n",
        "\n",
        "def list_model_path_elements(path):\n",
        "    try:\n",
        "        elements = os.listdir(path)\n",
        "        for element in elements:\n",
        "            print(element)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Call the function to list elements\n",
        "list_model_path_elements(model_path)\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "\n",
        "# Download and load stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def mask_word_tokens(text, tokenizer, mask_probability=0.40):\n",
        "    \"\"\"\n",
        "    Mask tokens randomly, excluding punctuation, stop words, and one-character words.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens_with_pos = pos_tag(tokens)\n",
        "\n",
        "    # Exclude stopwords, punctuation, and one-character words\n",
        "    eligible_tokens = [\n",
        "        i for i, (token, pos) in enumerate(tokens_with_pos)\n",
        "        if token.lower() not in stop_words and len(token) > 1 and token.isalnum()\n",
        "    ]\n",
        "\n",
        "    num_tokens_to_mask = max(1, int(len(eligible_tokens) * mask_probability))\n",
        "    mask_indices = random.sample(eligible_tokens, num_tokens_to_mask)\n",
        "\n",
        "    masked_tokens = tokens.copy()\n",
        "    for idx in mask_indices:\n",
        "        masked_tokens[idx] = tokenizer.mask_token\n",
        "\n",
        "    return tokenizer.convert_tokens_to_string(masked_tokens)\n",
        "\n",
        "def predict_masked_tokens(text_list, model, tokenizer):\n",
        "    for test_text in text_list:\n",
        "        masked_text = mask_word_tokens(test_text, tokenizer, mask_probability=0.10)\n",
        "        print(f\"Original text ---- {test_text}\")\n",
        "        print(f\"Masked text ---- {masked_text}\")\n",
        "\n",
        "        inputs = tokenizer(masked_text, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "\n",
        "        # Find the indices of the masked tokens\n",
        "        mask_token_indices = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "        # Decode the predicted tokens\n",
        "        predicted_tokens = []\n",
        "        for index in mask_token_indices:\n",
        "            predicted_token_id = logits[0, index].argmax(axis=-1)\n",
        "            predicted_token = tokenizer.decode(predicted_token_id)\n",
        "            predicted_tokens.append(predicted_token)\n",
        "\n",
        "        # Replace the [MASK] tokens with the predicted tokens\n",
        "        output_text = masked_text\n",
        "        for predicted_token in predicted_tokens:\n",
        "            output_text = output_text.replace('[MASK]', predicted_token, 1)\n",
        "\n",
        "        print(f\"Predicted text: {output_text}\")\n",
        "\n",
        "\n",
        "test_texts = [\n",
        "    \"Derivatives are financial contracts, set between two or more parties, that derive their value from an underlying asset, group of assets, or benchmark.\"\n",
        "    ,\"To hedge, in finance, is to take an offsetting position in an asset or investment that reduces the price risk of an existing position. A hedge is therefore a trade that is made with the purpose of reducing the risk of adverse price movements in another asset. Normally, a hedge consists of taking the opposite position in a related security or in a derivative security based on the asset to be hedged.\"\n",
        "    ,\"Financial exposure is the amount an investor stands to lose in an investment should the investment fail. For example, the financial exposure involved in purchasing a car would be the initial investment amount minus the insured portion. Knowing and understanding financial exposure, which is an alternative name for risk, is a crucial part of the investment process.\"\n",
        "    ,\"Unsecured Debt Definition: Unsecured debts are loans that are not collateralized. They generally require higher interest rates because they offer the lender limited protection against default. Lenders can mitigate this risk by reporting defaults to credit rating agencies.\"\n",
        "    ,\"Market capitalization, or market cap, represents the total dollar market value of a company's outstanding shares of stock. Investors use this figure to determine a company's size instead of sales or total asset value. In an acquisition, the market cap helps determine whether a takeover candidate represents a good value for the acquirer.\"\n",
        "]\n",
        "\n",
        "predict_masked_tokens(test_texts, model, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tJutB8-WRzf",
        "outputId": "e595aca2-7ed1-4030-9144-db4e098b1d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.safetensors\n",
            "vocab.txt\n",
            "special_tokens_map.json\n",
            "tokenizer.json\n",
            "tokenizer_config.json\n",
            "nlp_lstm_finetuned.pth\n",
            "config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text ---- Derivatives are financial contracts, set between two or more parties, that derive their value from an underlying asset, group of assets, or benchmark.\n",
            "Masked text ---- derivatives are financial [MASK], set between two or more parties, that derive their value from an underlying asset, group of assets, or benchmark.\n",
            "Predicted text: derivatives are financial instruments, set between two or more parties, that derive their value from an underlying asset, group of assets, or benchmark.\n",
            "Original text ---- To hedge, in finance, is to take an offsetting position in an asset or investment that reduces the price risk of an existing position. A hedge is therefore a trade that is made with the purpose of reducing the risk of adverse price movements in another asset. Normally, a hedge consists of taking the opposite position in a related security or in a derivative security based on the asset to be hedged.\n",
            "Masked text ---- to hedge, in finance, is to take an offsetting position in an asset or investment that reduces the price risk of an [MASK] position. a [MASK] is therefore a trade that is made with the purpose of reducing the risk of adverse price movements in [MASK] asset. normally, a hedge consists of taking the opposite position in a related security or in a derivative security based on the asset to be hedged.\n",
            "Predicted text: to hedge, in finance, is to take an offsetting position in an asset or investment that reduces the price risk of an underlying position. a hedge is therefore a trade that is made with the purpose of reducing the risk of adverse price movements in an asset. normally, a hedge consists of taking the opposite position in a related security or in a derivative security based on the asset to be hedged.\n",
            "Original text ---- Financial exposure is the amount an investor stands to lose in an investment should the investment fail. For example, the financial exposure involved in purchasing a car would be the initial investment amount minus the insured portion. Knowing and understanding financial exposure, which is an alternative name for risk, is a crucial part of the investment process.\n",
            "Masked text ---- financial exposure is the amount an investor [MASK] to lose in an investment should the investment fail. for example, the financial exposure involved in purchasing a car [MASK] be the [MASK] investment amount minus the insured portion. knowing and understanding financial exposure, which is an alternative name for risk, is a crucial part of the investment process.\n",
            "Predicted text: financial exposure is the amount an investor has to lose in an investment should the investment fail. for example, the financial exposure involved in purchasing a car should be the total investment amount minus the insured portion. knowing and understanding financial exposure, which is an alternative name for risk, is a crucial part of the investment process.\n",
            "Original text ---- Unsecured Debt Definition: Unsecured debts are loans that are not collateralized. They generally require higher interest rates because they offer the lender limited protection against default. Lenders can mitigate this risk by reporting defaults to credit rating agencies.\n",
            "Masked text ---- unsecured debt definition : unsecured debts are loans that are not [MASK]ized. they generally require higher interest rates because they [MASK] the lender limited protection against default. lenders can mitigate this risk by reporting defaults to credit rating agencies.\n",
            "Predicted text: unsecured debt definition : unsecured debts are loans that are not creditized. they generally require higher interest rates because they give the lender limited protection against default. lenders can mitigate this risk by reporting defaults to credit rating agencies.\n",
            "Original text ---- Market capitalization, or market cap, represents the total dollar market value of a company's outstanding shares of stock. Investors use this figure to determine a company's size instead of sales or total asset value. In an acquisition, the market cap helps determine whether a takeover candidate represents a good value for the acquirer.\n",
            "Masked text ---- market capitalization, or market cap, represents the total dollar market value of a company ' s outstanding shares of stock. [MASK] use this figure to determine a company ' s size instead of [MASK] or total asset value. in an acquisition, the market cap helps determine whether a takeover [MASK] represents a good value for the acquirer.\n",
            "Predicted text: market capitalization, or market cap, represents the total dollar market value of a company ' s outstanding shares of stock. investors use this figure to determine a company ' s size instead of market or total asset value. in an acquisition, the market cap helps determine whether a takeover bid represents a good value for the acquirer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1Ia_4o4JJDoyIkPcbMC_eZbWE_1DpuerL'\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLsktlVJbMbi",
        "outputId": "daa33add-7d7f-47af-8f67-5d04e8d2b1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ia_4o4JJDoyIkPcbMC_eZbWE_1DpuerL\n",
            "To: /content/data.zip\n",
            "100%|██████████| 12.2M/12.2M [00:00<00:00, 25.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rFw-vkZbtqp",
        "outputId": "b0222db1-72e3-4eaa-b1a6-5d947d53036d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.2-py3-none-any.whl (973 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/973.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.6/973.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.4-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.4/310.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.74-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.2 langchain-core-0.2.4 langchain-text-splitters-0.2.1 langsmith-0.1.74 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch.nn as nn\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load dataset\n",
        "csv_file = '/content/data/updated_file_pre_2006.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Load the tokenizer\n",
        "model_path = \"/content/models\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Preprocess the sample content\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \"]\n",
        ")\n",
        "\n",
        "def split_text(text):\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# Load the trained LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add batch dimension for LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = lstm_out[:, -1, :]  # Get the last output of LSTM\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "input_dim = 768  # The dimension of the embeddings\n",
        "hidden_dim = 128\n",
        "output_dim = 2  # Positive or Negative\n",
        "num_layers = 2\n",
        "\n",
        "lstm_model = LSTMClassifier(input_dim, hidden_dim, output_dim, num_layers)\n",
        "lstm_model.load_state_dict(torch.load('/content/models/nlp_lstm_finetuned.pth'))\n",
        "lstm_model.to(device)\n",
        "lstm_model.eval()\n",
        "\n",
        "# Function to get CLS embeddings for the sample chunks\n",
        "def get_cls_embeddings(texts, tokenizer, model, device):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
        "    return cls_embeddings.cpu().numpy()\n",
        "\n",
        "# Label encoder for the actual classification\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit([0, 1])  # Assuming 0 for Negative and 1 for Positive\n",
        "\n",
        "# Function to test the model accuracy\n",
        "def test_model_accuracy(data, num_samples, tokenizer, model, lstm_model, device):\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sample_content = data['content'].iloc[i]\n",
        "        actual_classification = data['Classification'].iloc[i]\n",
        "\n",
        "        sample_chunks = split_text(sample_content)\n",
        "\n",
        "        # Get embeddings for the sample chunks\n",
        "        sample_embeddings = get_cls_embeddings(sample_chunks, tokenizer, model, device)\n",
        "\n",
        "        # Prepare the embeddings for the LSTM model\n",
        "        sample_embeddings_tensor = torch.tensor(sample_embeddings, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Predict the classification\n",
        "        with torch.no_grad():\n",
        "            outputs = lstm_model(sample_embeddings_tensor)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predicted_label = predicted.cpu().numpy()[0]\n",
        "\n",
        "        # Transform actual classification label\n",
        "        actual_label_transformed = 1 if actual_classification == 1.0 else 0\n",
        "\n",
        "        # Check if the prediction is correct\n",
        "        if predicted_label == actual_label_transformed:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        # Print the classification\n",
        "        classification = label_encoder.inverse_transform([predicted_label])[0]\n",
        "        print(f\"Content: {sample_content[:100]}...\")  # Print first 100 characters of content for brevity\n",
        "        print(f\"Actual Classification: {'Positive' if actual_label_transformed == 1 else 'Negative'}\")\n",
        "        print(f\"Predicted Classification: {'Positive' if classification == 1 else 'Negative'}\\n\")\n",
        "\n",
        "    accuracy = correct_predictions / num_samples\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Assert if accuracy is at least 80%\n",
        "    assert accuracy >= 0.80, f\"Test failed with accuracy {accuracy * 100:.2f}%\"\n",
        "\n",
        "# Run the test\n",
        "test_model_accuracy(data, num_samples=10, tokenizer=tokenizer, model=model, lstm_model=lstm_model, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGqLR9ukVczH",
        "outputId": "15e37706-7431-4e45-c6cb-dddae3637132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at /content/models and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content: I discovered when I joined the Board of Governors of the Federal Reserve System about six months ago...\n",
            "Actual Classification: Negative\n",
            "Predicted Classification: Negative\n",
            "\n",
            "Content: I am privileged to accept the Union League of Philadelphia's Abraham Lincoln award.  This is the fir...\n",
            "Actual Classification: Positive\n",
            "Predicted Classification: Positive\n",
            "\n",
            "Content: The Challenge of Central Banking in a Democratic SocietyGood evening ladies and gentlemen.  I am esp...\n",
            "Actual Classification: Positive\n",
            "Predicted Classification: Negative\n",
            "\n",
            "Content: It is a pleasure to be with you this morning to discuss private-sector payments risk management in o...\n",
            "Actual Classification: Positive\n",
            "Predicted Classification: Positive\n",
            "\n",
            "Content: It is a pleasure to be here and participate in your discussions of current changes in bank regulator...\n",
            "Actual Classification: Positive\n",
            "Predicted Classification: Negative\n",
            "\n",
            "Content: The Transformation of the U.S. Banking Industry and Resulting Challenges to RegulatorsGood morning. ...\n",
            "Actual Classification: Negative\n",
            "Predicted Classification: Negative\n",
            "\n",
            "Content: Banking in the Global MarketplaceIt is again a pleasure to be here in Tokyo at the invitation of the...\n",
            "Actual Classification: Positive\n",
            "Predicted Classification: Positive\n",
            "\n",
            "Content: The Future of Electronic PaymentsI am delighted to be with you this morning to discuss the future of...\n",
            "Actual Classification: Negative\n",
            "Predicted Classification: Negative\n",
            "\n",
            "Content: It is a pleasure to join you today for the 23rd Annual Accounting Lecture Series at the University o...\n",
            "Actual Classification: Positive\n",
            "Predicted Classification: Positive\n",
            "\n",
            "Content: It is a pleasure to be with you this evening, and accept your honorary award.\n",
            "The Conference Board h...\n",
            "Actual Classification: Negative\n",
            "Predicted Classification: Negative\n",
            "\n",
            "Accuracy: 80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load dataset\n",
        "csv_file = '/content/data/updated_file_pre_2006.csv'\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Ensure NLTK's punkt tokenizer models are available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Calculate the average number of tokens in the 'content' column\n",
        "data['token_count'] = data['content'].apply(lambda x: len(word_tokenize(str(x))))\n",
        "average_tokens = data['token_count'].mean()\n",
        "\n",
        "print(f\"The average number of tokens in the 'content' column is {average_tokens}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQDkWZy-e8U-",
        "outputId": "4fb4f71c-45ed-4007-f25e-92eb517ef158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average number of tokens in the 'content' column is 43496.335913312694\n"
          ]
        }
      ]
    }
  ]
}